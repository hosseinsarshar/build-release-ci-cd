{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ML Pipeline at DevOps Build Pipeline\n",
    "\n",
    "In the prevous section we learned how to create, publish and schedule an ML pipeline. But from the initial diagram (Displayed below) we want to generate an ML Pipeline when ever there is a new code in the master branch.\n",
    "\n",
    "<img src=\"assets/MLOpsArchFlow.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we aim to make some modifications to the previous notebook (MLPipeline_MNIST) so that Azure DevOps Build Pipeline can generate a new ML Pipeline every time the master branch of the GitHub repo is changed.\n",
    "\n",
    "This is an important step to build a fully automated CI/CD pipeline for our ML project. So the scenario works like this:\n",
    "\n",
    "As a new code hits the master branch (this time we like to trigger the build Pipeline at the CI \"merge into the Master branch\") that hosts our code for the training pipeline, we like to execute the code to generate a new ML Pipeline with the new code. The ML Pipeline then generates a new ML model. The ML models is evaluated and if the accuracy is higher than the existing model, it is pushed into production.\n",
    "\n",
    "One major difference in this scenario is that we have to generate the ML Pipeline from the Ubunto computer within Azure DevOps. That computer doesn't have access to our Azure's subscription and also we don't want to manually go through the authentication process. We want this to be automatic. Therefore, we need to create a mechanisim that the machine can log in in absence of us to access our Azure environment and in particular our Azure Workspace.\n",
    "\n",
    "One way to do this is to create a user name of type Service Principle. This user name is designed to let applications authenticate into Azure. So first we need to create a Service Principle Account. The steps are provided here: https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal\n",
    "\n",
    "Save the following pieces of information: **Application ID**, **Tenant ID**, **Secret Key** and replace them in the code below:\n",
    "\n",
    "1. Create an Azure Active Directory application\n",
    "2. Assign the application to a role\n",
    "3. Get values for signing in\n",
    "4. Certificates and secrets -> Create a new application secret\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important points from the previous session:**\n",
    "* Pay attention to the requirements.txt as we only need to generate an ML Pipeline, azureml-sdk is the only package we need\n",
    "* Pay attention to the tests folder. Make sure you have at least 1 test under tests folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like always we import some packages related to the Azure ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.65\n",
      "Pipeline SDK-specific imports completed\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenant_id = \"<tenant_id>\"\n",
    "application_id = \"<application_id>\"\n",
    "object_id = \"<object_id>\"\n",
    "subscription_id = \"<subscription_id>\"\n",
    "app_secret = \"<app_secret>\"\n",
    "resource_group = \"<resource_group>\"\n",
    "workspace_name = \"<workspace_name>\"\n",
    "workspace_region = \"<workspace_region>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Service Principal Identity makes the code capable of accessing to our Azure Environment and Access the ML Workspace. In this case, it can create the Pipeline through the Build Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you download the file as Python and push the changes to the source repo. Now, we're ready to create our next gen Build Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "service_principal = ServicePrincipalAuthentication(\n",
    "        tenant_id=tenant_id,\n",
    "        service_principal_id=application_id,\n",
    "        service_principal_password=app_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.get(\n",
    "            name=workspace_name,\n",
    "            subscription_id=subscription_id,\n",
    "            resource_group=resource_group,\n",
    "            auth=service_principal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blobstore's name: workspaceblobstore\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the pointer to the default Blob storage.\n",
    "\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataReference object created\n"
     ]
    }
   ],
   "source": [
    "blob_input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"mnist_datainput\",\n",
    "    path_on_datastore=\"mnist_datainput\")\n",
    "\n",
    "print(\"DataReference object created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-10-17T03:23:32.008000+00:00', 'errors': None, 'creationTime': '2019-10-17T03:21:57.405412+00:00', 'modifiedTime': '2019-10-17T03:22:14.075980+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': ''}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D2_V2'}\n"
     ]
    }
   ],
   "source": [
    "# Create a GPU cluster of type NV6 with 1 node. (due to subscription's limitations we stick to 1 node)\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target_cpu = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=1,\n",
    "                                                           min_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target_cpu = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target_cpu.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target_cpu.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2019-10-17T01:25:40.719000+00:00', 'errors': None, 'creationTime': '2019-10-17T01:24:02.664673+00:00', 'modifiedTime': '2019-10-17T01:24:18.795336+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 1, 'maxNodeCount': 1, 'nodeIdleTimeBeforeScaleDown': ''}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NV6'}\n"
     ]
    }
   ],
   "source": [
    "# choose a name for your cluster\n",
    "cluster_name = \"gpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target_gpu = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV6', \n",
    "                                                           max_nodes=1,\n",
    "                                                           min_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target_gpu = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target_gpu.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target_gpu.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpucluster\n",
      "cpucluster\n"
     ]
    }
   ],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_processed_mnist_data"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_mnist_data = PipelineData(\"processed_mnist_data\", datastore=def_blob_store)\n",
    "processed_mnist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                                          'numpy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Extraction Step created\n"
     ]
    }
   ],
   "source": [
    "# source directory\n",
    "source_directory = 'DataExtraction'\n",
    "\n",
    "extractDataStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\", \n",
    "    arguments=[\"--output_extract\", processed_mnist_data],\n",
    "    outputs=[processed_mnist_data],\n",
    "    compute_target=compute_target_cpu, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config)\n",
    "\n",
    "print(\"Data Extraction Step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "source_directory = 'Training'\n",
    "est = TensorFlow(source_directory=source_directory,\n",
    "                 compute_target=compute_target_gpu,\n",
    "                 entry_script='train.py', \n",
    "                 use_gpu=True, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training Step is Completed\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.steps import EstimatorStep\n",
    "\n",
    "model_name = \"tf_mnist_pipeline_devops.model\"\n",
    "trainingStep = EstimatorStep(name=\"Training-Step\",\n",
    "                             estimator=est,\n",
    "                             estimator_entry_script_arguments=[\"--input_data_location\", processed_mnist_data,\n",
    "                                                               '--batch-size', 50,\n",
    "                                                               '--first-layer-neurons', 300,\n",
    "                                                               '--second-layer-neurons', 100,\n",
    "                                                               '--learning-rate', 0.01,\n",
    "                                                               \"--release_id\", 0,\n",
    "                                                               '--model_name', model_name],\n",
    "                             runconfig_pipeline_params=None,\n",
    "                             inputs=[processed_mnist_data],\n",
    "                             compute_target=compute_target_gpu)\n",
    "\n",
    "print(\"Model Training Step is Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation and Registration Step is Created\n"
     ]
    }
   ],
   "source": [
    "# source directory\n",
    "source_directory = 'RegisterModel'\n",
    "\n",
    "modelEvalReg = PythonScriptStep(\n",
    "    name=\"Evaluate and Register Model\",\n",
    "    script_name=\"evaluate_model.py\", \n",
    "    arguments=[\"--release_id\", 0,\n",
    "               '--model_name', model_name],\n",
    "    compute_target=compute_target_cpu, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config)\n",
    "\n",
    "modelEvalReg.run_after(trainingStep)\n",
    "print(\"Model Evaluation and Registration Step is Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step extract.py [58230a66][ad2254cc-a96e-4d66-8cc7-904bc65d13dd], (This step is eligible to reuse a previous run's output)\n",
      "Created step Training-Step [2443bfff][c7915519-a64f-475c-b17c-6477efb5fd34], (This step is eligible to reuse a previous run's output)\n",
      "Created step Evaluate and Register Model [ed322431][818c6122-abcb-48a2-af2d-b6ae3b3e2b60], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 54f101bc-6c89-4f94-a1bf-41179d96cf0b\n",
      "Link to Azure Portal: https://mlworkspace.azure.ai/portal/subscriptions/ed70929a-e125-4daf-8945-04f709d2c75e/resourceGroups/MLOpsWorkshop/providers/Microsoft.MachineLearningServices/workspaces/FirstExample/experiments/MNIST-Model-Training/runs/54f101bc-6c89-4f94-a1bf-41179d96cf0b\n"
     ]
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "pipeline = Pipeline(workspace=ws, steps=[extractDataStep, trainingStep, modelEvalReg])\n",
    "pipeline_run = Experiment(ws, 'MNIST-Model-Training-Build-CI').submit(pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True, raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name=\"MNIST-Pipeline-Created-At-Build-Pipeline\", \n",
    "                                                   description=\"Steps are: data preparation, training, model validation and model registration\", \n",
    "                                                   version=\"0.1\", \n",
    "                                                   continue_on_step_failure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}