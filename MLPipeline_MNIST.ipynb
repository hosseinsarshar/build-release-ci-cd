{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipelines\n",
    "\n",
    "A Machine Learning pipeline is a set of independent steps that each can have different compute, data, environment, etc. dependencies. Each step is treated as a separate run and all of the four aspects of a run (metrics, logs, snapshot and outputs) are stored independently. After the pipeline is created, it can be published as an API for later reuse.\n",
    "\n",
    "This gives you huge benefit as you have ultimate control over defining each step of the pipeline in a way that serves best for that particular step.\n",
    "\n",
    "ML Pipelines are some type of workflow orchestration tools. The main difference between ML Pipelines and regular job workflows (such as Azure Data Factory or Airflow) is that this workflow engine is designed to address ML needs. Therefore, if you have non-ML related workflow it's recommended you use generic workflow engines. You may call an ML Pipeline from a generic pipeline as a step.\n",
    "\n",
    "In this tutorial, we want to treat the MNIST model we trained in the previous example like a serious ML problem. We'd like to break the task into three parts:\n",
    "1. Downloading the dataset from the Yann Lecun website, unzip and normalize it and save it in a shared storage\n",
    "2. Train our 2-Layer Neural Net on the normalized data\n",
    "3. Register the model in case the performance of the new model is higher than the latest version of the model in the Model Registry\n",
    "4. Publish the pipeline as a bundle of the above steps as an API Endpoint\n",
    "\n",
    "At the end of this notebook, you'll build the following pipeline.\n",
    "![ML Pipeline](assets/MLOps_Pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining an ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, Datastore\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "print(\"Pipeline SDK-specific imports completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your subscription ID will be different replace the stirng with yours\n",
    "subscription_id = \"your_subscription_id\"\n",
    "resource_group = \"your_resource_group\"\n",
    "workspace_name = \"your_workspace_name\"\n",
    "workspace_region = \"your_workspace_region\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Workspace class and check the azureml SDK version\n",
    "# exist_ok checks if workspace exists or not.\n",
    "\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace(workspace_name = workspace_name,\n",
    "               subscription_id = subscription_id,\n",
    "               resource_group = resource_group)\n",
    "\n",
    "# persist the subscription id, resource group name, and workspace name in aml_config/config.json.\n",
    "ws.write_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Stores\n",
    "\n",
    "As described in the previous section, Datastores are attached to workspaces and are used to store connection information to Azure storage services so you can refer to them by name and don't need to remember the connection information and secret used to connect to the storage services.\n",
    "\n",
    "https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore the registered Datastores in your Workspace, login to **ml.azure.com**, in the left pane, under **Manage** section click on the **Datastores**. By default, you have two data stores registered, **workspaceblobstore** and **workspacefilestore**.\n",
    "\n",
    "We skip the **workspacefilestore** for now and only use **workspaceblobstore** for this exercise. **workspaceblobstore** is referring to the default Blob storage that is created at the creation of Workspace. **Blob storage** is a type of storage account that is used to keep any type of data from binary (image files) to csv or parquet (It's similar to **AWS S3**). At any time you can register a new Azure's storage account at your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the pointer to the default Blob storage.\n",
    "\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "\n",
    "## The code below also yields the same result:\n",
    "# def_blob_store = ws.get_default_datastore()\n",
    "\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An object from DataReference class represents a path within a Datastore. So in the example below, you're explaining that the MNIST data should be available in the **mnist_datainput** parth under the **workspaceblobstore** container in the Azure storage account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob_input_data = DataReference(\n",
    "#     datastore=def_blob_store,\n",
    "#     data_reference_name=\"mnist_datainput\",\n",
    "#     path_on_datastore=\"mnist_datainput\")\n",
    "# \n",
    "# print(\"DataReference object created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure the compute targets are created.\n",
    "\n",
    "In this example, we want to have two types of compute environment, the first compute type is a CPU type and the other is a GPU type cluster each with 1 node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPU cluster of type NV6 with 1 node. (due to subscription's limitations we stick to 1 node)\n",
    "\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"cpucluster\"\n",
    "\n",
    "try:\n",
    "    compute_target_cpu = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    # CPU: Standard_D3_v2\n",
    "    # GPU: Standard_NV6\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=1,\n",
    "                                                           min_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target_cpu = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target_cpu.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target_cpu.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Commenting this as we want to only use one compute target to make things faster. Try this later if you like to train on GPU.\n",
    "\n",
    "# # choose a name for your cluster\n",
    "# cluster_name = \"gpucluster\"\n",
    "# \n",
    "# try:\n",
    "#     compute_target_gpu = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "#     print('Found existing compute target.')\n",
    "# except ComputeTargetException:\n",
    "#     print('Creating a new compute target...')\n",
    "#     # CPU: Standard_D3_v2\n",
    "#     # GPU: Standard_NV6\n",
    "#     compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NV6', \n",
    "#                                                            max_nodes=1,\n",
    "#                                                            min_nodes=1)\n",
    "# \n",
    "#     # create the cluster\n",
    "#     compute_target_gpu = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "# \n",
    "#     compute_target_gpu.wait_for_completion(show_output=True)\n",
    "# \n",
    "# # use get_status() to get a detailed status for the current cluster. \n",
    "# print(compute_target_gpu.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PipelineData is a way to define data dependancies in an ML Pipeline. In this example, we want to first download the MNIST data into a directory called raw_data and then save the processed and normalized numpy objects into a subdirectory called Processed. This PipelineData object will be used as the output of the first step named Data Extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_mnist_data = PipelineData(\"processed_mnist_data\", datastore=def_blob_store)\n",
    "processed_mnist_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step is a regular Python script, and can be executed on a CPU node with no prepackaged ML environment requirment, we stick to the default configurations.\n",
    "\n",
    "The configurations below, first deploys a CPU based linux docker image on the VM and then installs 'azureml-sdk' and 'numpy' packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# enable Docker \n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "# set Docker base image to the default CPU-based image\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(pip_packages=['azureml-sdk',\n",
    "                                                                                          'numpy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the first step by defining an object from PythonScriptStep class. Under the hood, it calls the extract.py file as the entry script and we pass the **processed_mnist_data** object as a parameter to the script.\n",
    "\n",
    "**outputs** parameter defines the data output dependencies that in this case, we have an object of DataPipeline **processed_mnist_data** as the output dependency.\n",
    "\n",
    "As the script can run on a CPU node, we don't waste our money by running it on a GPU node. Therefore, we select **compute_target_cpu** as the target compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source directory\n",
    "source_directory = 'DataExtraction'\n",
    "\n",
    "extractDataStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\", \n",
    "    arguments=[\"--output_extract\", processed_mnist_data],\n",
    "    outputs=[processed_mnist_data],\n",
    "    compute_target=compute_target_cpu, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config)\n",
    "\n",
    "print(\"Data Extraction Step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run our Tensorflow job to train our very best MNIST classifier. As this is a TF job, we can leverage Estimator classes such as  **azureml.train.dnn.TensorFlow**. Moreover, the inputs argument instructs the Pipeline what should be the dependency before executing this step. As the **processed_mnist_data** PipelineData object is provided as the output for the step above and input for this step, the Pipeline engine will execute the Training step after the Data Extraction step.\n",
    "\n",
    "As a TF job, we can leverage our GPU node to boost up the computational performance of the training step. So we provide the GPU cluster as the compute target.\n",
    "\n",
    "The TF estimator support TF 1. If your script is based on TF 2, then you can use the **PythonScriptStep** and provide your custom docker image or pip install TF 2 on a base GPU image.\n",
    "\n",
    "We provide two arguments, **release_id** and **model_name**. **release_id** helps us to logically tag the run to a number that later can be retrieved. You can think of the **release_id** as the version number. In the 3rd day, you'll use the release pipeline to populate the release_id. **model_name** as it's name suggests instructs the code on what name to use to save the model in the run->output section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "source_directory = 'Training'\n",
    "est = TensorFlow(source_directory=source_directory,\n",
    "                 compute_target=compute_target_cpu,\n",
    "                 entry_script='train.py', \n",
    "                 use_gpu=False, \n",
    "                 framework_version='1.13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import EstimatorStep\n",
    "\n",
    "trainingStep = EstimatorStep(name=\"Training-Step\",\n",
    "                             estimator=est,\n",
    "                             estimator_entry_script_arguments=[\"--input_data_location\", processed_mnist_data,\n",
    "                                                               '--batch-size', 50,\n",
    "                                                               '--first-layer-neurons', 300,\n",
    "                                                               '--second-layer-neurons', 100,\n",
    "                                                               '--learning-rate', 0.01,\n",
    "                                                               \"--release_id\", 0,\n",
    "                                                               '--model_name', 'tf_mnist_pipeline.model'],\n",
    "                             runconfig_pipeline_params=None,\n",
    "                             inputs=[processed_mnist_data],\n",
    "                             compute_target=compute_target_cpu)\n",
    "\n",
    "print(\"Model Training Step is Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we want to evaluate and register the model. Similar to the first step, we only need a CPU node to accomplish this task as we're running a regular python script with no ML dependencies. So we instantiate an object from **PythonScriptStep** class and provide **evaluate_model.py** as the entry script. \n",
    "\n",
    "The two arguments we provided in the step above are used here to retrieve the model saved in the run->output section of the experiment. Using release id, we can retrieve all other models and check if this model is outperforming them or not. If not we don't register this model into the model registry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# source directory\n",
    "source_directory = 'RegisterModel'\n",
    "\n",
    "modelEvalReg = PythonScriptStep(\n",
    "    name=\"Evaluate and Register Model\",\n",
    "    script_name=\"evaluate_model.py\", \n",
    "    arguments=[\"--release_id\", 0,\n",
    "               '--model_name', 'tf_mnist_pipeline.model'],\n",
    "    compute_target=compute_target_cpu, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config)\n",
    "print(\"Model Evaluation and Registration Step is Created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this step doesn't have any inputs or outputs data dependancies, the Pipeline engine will execute it in parallel with the prevous two steps. However, logically we should execute this after the training step. Therefore, we use the below command the instruct the Pipeline engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEvalReg.run_after(trainingStep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all of the pipeline steps are defined. We can build the Pipeline class which defines how the pipeline should be executed.\n",
    "\n",
    "Pipelines are loosely coupled with Experiments. At run time you can define to which Experiment this pipeline execution should be connected. For this we define/connect to **MNIST-Model-Manual-Pipeline** experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "pipeline = Pipeline(workspace=ws, steps=[extractDataStep, trainingStep, modelEvalReg])\n",
    "pipeline_run = Experiment(ws, 'MNIST-Model-Manual-Pipeline').submit(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to any run, Pipeline runs are non-blocking. However, if you automate this job, you like your code to wait until the pipeline is constructed and executed. So you'd use the below command to make the run execution blocking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True, raise_on_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Publish and trigger a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Pipeline is executed once under an experiment. But for later use, you may want to Publish the Pipeline as an Endpoint. publish_pipeline method publishes a pipeline under Pipeline section of Workspace. The published pipeline can later be called from any where inside or outside of Azure.\n",
    "\n",
    "One of the use-cases is to call the Pipeline within a Data Engineering Pipeline. So the Data Engineering team can trigger the piblished pipeline by having the URI information of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(name=\"MNIST-Pipeline-Manually-Built-Manulife\", \n",
    "                                                   description=\"Steps are: data preparation, training, model validation and model registration\", \n",
    "                                                   version=\"0.1\", \n",
    "                                                   continue_on_step_failure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PublishedPipeline\n",
    "\n",
    "pipeline_id = published_pipeline.id # use your published pipeline id\n",
    "published_pipeline = PublishedPipeline.get(ws, pipeline_id)\n",
    "published_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the endpoint that is callable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_endpoint = published_pipeline.endpoint\n",
    "rest_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can call the Endpoint from anywhere. In order to call the endpoint, you need to authenticate yourself. There are two ways to do that:\n",
    "    \n",
    "    1. InteractiveLoginAuthentication\n",
    "    1. ServicePrincipalAuthentication\n",
    "    \n",
    "The first one requires you to authentical yourself interactively or be already authenticated. As we're already authenticated, so we can use the first approach. The second approach id described in the next section.\n",
    "\n",
    "The InteractiveLoginAuthentication class can help us generate the authentication key required to connect with the ML Pipeline Endpoint using **get_authentication_header** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "import requests\n",
    "\n",
    "auth = InteractiveLoginAuthentication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad_token = auth.get_authentication_header()\n",
    "aad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally using a simple post request, you can trigger the pipeline. Post request are available in any modern programming language, therefore, you can trigger the pipeline from anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the param when running the pipeline\n",
    "response = requests.post(rest_endpoint, \n",
    "                         headers=aad_token, \n",
    "                         json={\"ExperimentName\": \"Kicked_MNist_Pipeline_Remotely\",\n",
    "                               \"RunSource\": \"SDK\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = response.json()[\"Id\"]\n",
    "\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the Pipeline Run:\n",
    "\n",
    "from azureml.pipeline.core import PipelineRun\n",
    "\n",
    "exp = Experiment(name=\"Kicked_MNist_Pipeline_Remotely\", workspace=ws)\n",
    "pipeline_run = PipelineRun(experiment=exp, run_id=run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Schedule the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of using a Pipeline is to schedule it. In the example below, the pipeline is scheduled to be triggered weekly on Fridays at 15:30 UTC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from azureml.pipeline.core import Schedule, ScheduleRecurrence\n",
    "# \n",
    "# \n",
    "# recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, week_days=[\"Friday\"], time_of_day=\"15:30\")\n",
    "# schedule = Schedule.create(ws, name=\"ScheduledPipeline\", pipeline_id=pipeline_id,\n",
    "#                               experiment_name=\"MNIST-Pipeline-Wekly-Scheduled\", recurrence=recurrence)\n",
    "# \n",
    "## Get the list of scheduled Pipelines\n",
    "# Schedule.list(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add . && git commit -m \"Pipeline published and scheduled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete compute resources\n",
    "\n",
    "Once this exercise is completed, you can delete disposable resources so you avoid getting unnecessary charges.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target_cpu.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
